{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS():\n",
    "    def __init__(self):\n",
    "        self.adjoint = True\n",
    "        self.viz = True\n",
    "        self.niters = 1000\n",
    "        self.lr = 1e-3\n",
    "        self.num_samples = 512\n",
    "        self.width = 64\n",
    "        self.hidden_dim = 32\n",
    "        self.gpu = 0\n",
    "        self.train_dir = None\n",
    "        self.save = \"cnf\"\n",
    "        self.hidden_dim = 32\n",
    "\n",
    "class CNF(nn.Module):\n",
    "    \"\"\"Adapted from the NumPy implementation at:\n",
    "    https://gist.github.com/rtqichen/91924063aa4cc95e7ef30b3a5491cc52\n",
    "    \"\"\"\n",
    "    def __init__(self, in_out_dim, hidden_dim, width):\n",
    "        super().__init__()\n",
    "        self.in_out_dim = in_out_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.width = width\n",
    "        self.hyper_net = HyperNetwork(in_out_dim, hidden_dim, width)\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        z = states[0]\n",
    "        logp_z = states[1]\n",
    "\n",
    "        batchsize = z.shape[0]\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z.requires_grad_(True)\n",
    "\n",
    "            W, B, U = self.hyper_net(t)\n",
    "\n",
    "            Z = torch.unsqueeze(z, 0).repeat(self.width, 1, 1)\n",
    "\n",
    "            h = torch.tanh(torch.matmul(Z, W) + B)\n",
    "            dz_dt = torch.matmul(h, U).mean(0)\n",
    "\n",
    "            dlogp_z_dt = -trace_df_dz(dz_dt, z).view(batchsize, 1)\n",
    "            \n",
    "        return (dz_dt, dlogp_z_dt)\n",
    "\n",
    "\n",
    "def trace_df_dz(f, z):\n",
    "    \"\"\"Calculates the trace of the Jacobian df/dz.\n",
    "    Stolen from: https://github.com/rtqichen/ffjord/blob/master/lib/layers/odefunc.py#L13\n",
    "    \"\"\"\n",
    "    sum_diag = 0.\n",
    "    for i in range(z.shape[1]):\n",
    "        sum_diag += torch.autograd.grad(f[:, i].sum(), z, create_graph=True)[0].contiguous()[:, i].contiguous()\n",
    "\n",
    "    return sum_diag.contiguous()\n",
    "\n",
    "\n",
    "class HyperNetwork(nn.Module):\n",
    "    \"\"\"Hyper-network allowing f(z(t), t) to change with time.\n",
    "\n",
    "    Adapted from the NumPy implementation at:\n",
    "    https://gist.github.com/rtqichen/91924063aa4cc95e7ef30b3a5491cc52\n",
    "    \"\"\"\n",
    "    def __init__(self, in_out_dim, hidden_dim, width):\n",
    "        super().__init__()\n",
    "\n",
    "        blocksize = width * in_out_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 3 * blocksize + width)\n",
    "\n",
    "        self.in_out_dim = in_out_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.width = width\n",
    "        self.blocksize = blocksize\n",
    "\n",
    "    def forward(self, t):\n",
    "        # predict params\n",
    "        params = t.reshape(1, 1)\n",
    "        params = torch.tanh(self.fc1(params))\n",
    "        params = torch.tanh(self.fc2(params))\n",
    "        params = self.fc3(params)\n",
    "\n",
    "        # restructure\n",
    "        params = params.reshape(-1)\n",
    "        W = params[:self.blocksize].reshape(self.width, self.in_out_dim, 1)\n",
    "\n",
    "        U = params[self.blocksize:2 * self.blocksize].reshape(self.width, 1, self.in_out_dim)\n",
    "\n",
    "        G = params[2 * self.blocksize:3 * self.blocksize].reshape(self.width, 1, self.in_out_dim)\n",
    "        U = U * torch.sigmoid(G)\n",
    "\n",
    "        B = params[3 * self.blocksize:].reshape(self.width, 1, 1)\n",
    "        return [W, B, U]\n",
    "\n",
    "def get_batch(num_samples):\n",
    "    # points, _ = make_circles(n_samples=num_samples, noise=0.06, factor=0.5)\n",
    "    points, _ = make_moons(n_samples=num_samples, noise=0.06)\n",
    "    x = torch.tensor(points).type(torch.float32).to(device)\n",
    "    logp_diff_t1 = torch.zeros(num_samples, 1).type(torch.float32).to(device)\n",
    "\n",
    "    return(x, logp_diff_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1, running avg loss: 6.6777\n",
      "Iter: 2, running avg loss: 6.1141\n",
      "Iter: 3, running avg loss: 5.6708\n",
      "Iter: 4, running avg loss: 5.2306\n",
      "Iter: 5, running avg loss: 4.8262\n",
      "Iter: 6, running avg loss: 4.4754\n",
      "Iter: 7, running avg loss: 4.0927\n",
      "Iter: 8, running avg loss: 3.7789\n",
      "Iter: 9, running avg loss: 3.4813\n",
      "Iter: 10, running avg loss: 3.1485\n",
      "Iter: 11, running avg loss: 2.9058\n",
      "Iter: 12, running avg loss: 2.6994\n",
      "Iter: 13, running avg loss: 2.4585\n",
      "Iter: 14, running avg loss: 2.3144\n",
      "Iter: 15, running avg loss: 2.1600\n",
      "Iter: 16, running avg loss: 2.0869\n",
      "Iter: 17, running avg loss: 2.0153\n",
      "Iter: 18, running avg loss: 1.9760\n",
      "Iter: 19, running avg loss: 1.9636\n",
      "Iter: 20, running avg loss: 1.9679\n",
      "Iter: 21, running avg loss: 1.9635\n",
      "Iter: 22, running avg loss: 1.9585\n",
      "Iter: 23, running avg loss: 1.9587\n",
      "Iter: 24, running avg loss: 1.9633\n",
      "Iter: 25, running avg loss: 1.9505\n",
      "Iter: 26, running avg loss: 1.9541\n",
      "Iter: 27, running avg loss: 1.9456\n",
      "Iter: 28, running avg loss: 1.9321\n",
      "Iter: 29, running avg loss: 1.9254\n",
      "Iter: 30, running avg loss: 1.9263\n",
      "Iter: 31, running avg loss: 1.9102\n",
      "Iter: 32, running avg loss: 1.8981\n",
      "Iter: 33, running avg loss: 1.8910\n",
      "Iter: 34, running avg loss: 1.8932\n",
      "Iter: 35, running avg loss: 1.8717\n",
      "Iter: 36, running avg loss: 1.8694\n",
      "Iter: 37, running avg loss: 1.8670\n",
      "Iter: 38, running avg loss: 1.8550\n",
      "Iter: 39, running avg loss: 1.8554\n",
      "Iter: 40, running avg loss: 1.8560\n",
      "Iter: 41, running avg loss: 1.8454\n",
      "Iter: 42, running avg loss: 1.8498\n",
      "Iter: 43, running avg loss: 1.8536\n",
      "Iter: 44, running avg loss: 1.8329\n",
      "Iter: 45, running avg loss: 1.8325\n",
      "Iter: 46, running avg loss: 1.8279\n",
      "Iter: 47, running avg loss: 1.8316\n",
      "Iter: 48, running avg loss: 1.8188\n",
      "Iter: 49, running avg loss: 1.8275\n",
      "Iter: 50, running avg loss: 1.8204\n",
      "Iter: 51, running avg loss: 1.8244\n",
      "Iter: 52, running avg loss: 1.8162\n",
      "Iter: 53, running avg loss: 1.8129\n",
      "Iter: 54, running avg loss: 1.8246\n",
      "Iter: 55, running avg loss: 1.8163\n",
      "Iter: 56, running avg loss: 1.8266\n",
      "Iter: 57, running avg loss: 1.8182\n",
      "Iter: 58, running avg loss: 1.8209\n",
      "Iter: 59, running avg loss: 1.8061\n",
      "Iter: 60, running avg loss: 1.8061\n",
      "Iter: 61, running avg loss: 1.8125\n",
      "Iter: 62, running avg loss: 1.8110\n",
      "Iter: 63, running avg loss: 1.8120\n",
      "Iter: 64, running avg loss: 1.8096\n",
      "Iter: 65, running avg loss: 1.8086\n",
      "Iter: 66, running avg loss: 1.7916\n",
      "Iter: 67, running avg loss: 1.8026\n",
      "Iter: 68, running avg loss: 1.7871\n",
      "Iter: 69, running avg loss: 1.7998\n",
      "Iter: 70, running avg loss: 1.7900\n",
      "Iter: 71, running avg loss: 1.7911\n",
      "Iter: 72, running avg loss: 1.7828\n",
      "Iter: 73, running avg loss: 1.7801\n",
      "Iter: 74, running avg loss: 1.7726\n",
      "Iter: 75, running avg loss: 1.7817\n",
      "Iter: 76, running avg loss: 1.7670\n",
      "Iter: 77, running avg loss: 1.7736\n",
      "Iter: 78, running avg loss: 1.7706\n",
      "Iter: 79, running avg loss: 1.7701\n",
      "Iter: 80, running avg loss: 1.7719\n",
      "Iter: 81, running avg loss: 1.7595\n",
      "Iter: 82, running avg loss: 1.7613\n",
      "Iter: 83, running avg loss: 1.7694\n",
      "Iter: 84, running avg loss: 1.7515\n",
      "Iter: 85, running avg loss: 1.7651\n",
      "Iter: 86, running avg loss: 1.7522\n",
      "Iter: 87, running avg loss: 1.7434\n",
      "Iter: 88, running avg loss: 1.7434\n",
      "Iter: 89, running avg loss: 1.7415\n",
      "Iter: 90, running avg loss: 1.7272\n",
      "Iter: 91, running avg loss: 1.7423\n",
      "Iter: 92, running avg loss: 1.7288\n",
      "Iter: 93, running avg loss: 1.7275\n",
      "Iter: 94, running avg loss: 1.7189\n",
      "Iter: 95, running avg loss: 1.7254\n",
      "Iter: 96, running avg loss: 1.7225\n",
      "Iter: 97, running avg loss: 1.7017\n",
      "Iter: 98, running avg loss: 1.7199\n",
      "Iter: 99, running avg loss: 1.7230\n",
      "Iter: 100, running avg loss: 1.7144\n",
      "Iter: 101, running avg loss: 1.7089\n",
      "Iter: 102, running avg loss: 1.7004\n",
      "Iter: 103, running avg loss: 1.6976\n",
      "Iter: 104, running avg loss: 1.7041\n",
      "Iter: 105, running avg loss: 1.6919\n",
      "Iter: 106, running avg loss: 1.6905\n",
      "Iter: 107, running avg loss: 1.6910\n",
      "Iter: 108, running avg loss: 1.6781\n",
      "Iter: 109, running avg loss: 1.6961\n",
      "Iter: 110, running avg loss: 1.6929\n",
      "Iter: 111, running avg loss: 1.6750\n",
      "Iter: 112, running avg loss: 1.6628\n",
      "Iter: 113, running avg loss: 1.6734\n",
      "Iter: 114, running avg loss: 1.6709\n",
      "Iter: 115, running avg loss: 1.6711\n",
      "Iter: 116, running avg loss: 1.6707\n",
      "Iter: 117, running avg loss: 1.6648\n",
      "Iter: 118, running avg loss: 1.6574\n",
      "Iter: 119, running avg loss: 1.6499\n",
      "Iter: 120, running avg loss: 1.6483\n",
      "Iter: 121, running avg loss: 1.6588\n",
      "Iter: 122, running avg loss: 1.6422\n",
      "Iter: 123, running avg loss: 1.6439\n",
      "Iter: 124, running avg loss: 1.6428\n",
      "Iter: 125, running avg loss: 1.6523\n",
      "Iter: 126, running avg loss: 1.6508\n",
      "Iter: 127, running avg loss: 1.6330\n",
      "Iter: 128, running avg loss: 1.6338\n",
      "Iter: 129, running avg loss: 1.6410\n",
      "Iter: 130, running avg loss: 1.6279\n",
      "Iter: 131, running avg loss: 1.6274\n",
      "Iter: 132, running avg loss: 1.6486\n",
      "Iter: 133, running avg loss: 1.6250\n",
      "Iter: 134, running avg loss: 1.6075\n",
      "Iter: 135, running avg loss: 1.6148\n",
      "Iter: 136, running avg loss: 1.6178\n",
      "Iter: 137, running avg loss: 1.6110\n",
      "Iter: 138, running avg loss: 1.6116\n",
      "Iter: 139, running avg loss: 1.6056\n",
      "Iter: 140, running avg loss: 1.6104\n",
      "Iter: 141, running avg loss: 1.6061\n",
      "Iter: 142, running avg loss: 1.6101\n",
      "Iter: 143, running avg loss: 1.5855\n",
      "Iter: 144, running avg loss: 1.6114\n",
      "Iter: 145, running avg loss: 1.5846\n",
      "Iter: 146, running avg loss: 1.5914\n",
      "Iter: 147, running avg loss: 1.5928\n",
      "Iter: 148, running avg loss: 1.5639\n",
      "Iter: 149, running avg loss: 1.5886\n",
      "Iter: 150, running avg loss: 1.5656\n",
      "Iter: 151, running avg loss: 1.5597\n",
      "Iter: 152, running avg loss: 1.5615\n",
      "Iter: 153, running avg loss: 1.5527\n",
      "Iter: 154, running avg loss: 1.5369\n",
      "Iter: 155, running avg loss: 1.5474\n",
      "Iter: 156, running avg loss: 1.5418\n",
      "Iter: 157, running avg loss: 1.5233\n",
      "Iter: 158, running avg loss: 1.5088\n",
      "Iter: 159, running avg loss: 1.5300\n",
      "Iter: 160, running avg loss: 1.5027\n",
      "Iter: 161, running avg loss: 1.4974\n",
      "Iter: 162, running avg loss: 1.4844\n",
      "Iter: 163, running avg loss: 1.4898\n",
      "Iter: 164, running avg loss: 1.4865\n",
      "Iter: 165, running avg loss: 1.4684\n",
      "Iter: 166, running avg loss: 1.4399\n",
      "Iter: 167, running avg loss: 1.4269\n",
      "Iter: 168, running avg loss: 1.4074\n",
      "Iter: 169, running avg loss: 1.4205\n",
      "Iter: 170, running avg loss: 1.4059\n",
      "Iter: 171, running avg loss: 1.4115\n",
      "Iter: 172, running avg loss: 1.3790\n",
      "Iter: 173, running avg loss: 1.3692\n",
      "Iter: 174, running avg loss: 1.3767\n",
      "Iter: 175, running avg loss: 1.3598\n",
      "Iter: 176, running avg loss: 1.3482\n",
      "Iter: 177, running avg loss: 1.3403\n",
      "Iter: 178, running avg loss: 1.3486\n",
      "Iter: 179, running avg loss: 1.3212\n",
      "Iter: 180, running avg loss: 1.3459\n",
      "Iter: 181, running avg loss: 1.3224\n",
      "Iter: 182, running avg loss: 1.2979\n",
      "Iter: 183, running avg loss: 1.3025\n",
      "Iter: 184, running avg loss: 1.3022\n",
      "Iter: 185, running avg loss: 1.3111\n",
      "Iter: 186, running avg loss: 1.2844\n",
      "Iter: 187, running avg loss: 1.2811\n",
      "Iter: 188, running avg loss: 1.2704\n",
      "Iter: 189, running avg loss: 1.3072\n",
      "Iter: 190, running avg loss: 1.2644\n",
      "Iter: 191, running avg loss: 1.3067\n",
      "Iter: 192, running avg loss: 1.2366\n",
      "Iter: 193, running avg loss: 1.2524\n",
      "Iter: 194, running avg loss: 1.2421\n",
      "Iter: 195, running avg loss: 1.2376\n",
      "Iter: 196, running avg loss: 1.2633\n",
      "Iter: 197, running avg loss: 1.2181\n",
      "Iter: 198, running avg loss: 1.2458\n",
      "Iter: 199, running avg loss: 1.1988\n",
      "Iter: 200, running avg loss: 1.2249\n",
      "Iter: 201, running avg loss: 1.2164\n",
      "Iter: 202, running avg loss: 1.2095\n",
      "Iter: 203, running avg loss: 1.2228\n",
      "Iter: 204, running avg loss: 1.2118\n",
      "Iter: 205, running avg loss: 1.1830\n",
      "Iter: 206, running avg loss: 1.2015\n",
      "Iter: 207, running avg loss: 1.2001\n",
      "Iter: 208, running avg loss: 1.1713\n",
      "Iter: 209, running avg loss: 1.1601\n",
      "Iter: 210, running avg loss: 1.1703\n",
      "Iter: 211, running avg loss: 1.1629\n",
      "Iter: 212, running avg loss: 1.1460\n",
      "Iter: 213, running avg loss: 1.1994\n",
      "Iter: 214, running avg loss: 1.1792\n",
      "Iter: 215, running avg loss: 1.2020\n",
      "Iter: 216, running avg loss: 1.1116\n",
      "Iter: 217, running avg loss: 1.2361\n",
      "Iter: 218, running avg loss: 1.1312\n",
      "Iter: 219, running avg loss: 1.1768\n",
      "Iter: 220, running avg loss: 1.1017\n",
      "Iter: 221, running avg loss: 1.2110\n",
      "Iter: 222, running avg loss: 1.0989\n",
      "Iter: 223, running avg loss: 1.1442\n",
      "Iter: 224, running avg loss: 1.0910\n",
      "Iter: 225, running avg loss: 1.1684\n",
      "Iter: 226, running avg loss: 1.1218\n",
      "Iter: 227, running avg loss: 1.1118\n",
      "Iter: 228, running avg loss: 1.1393\n",
      "Iter: 229, running avg loss: 1.1052\n",
      "Iter: 230, running avg loss: 1.1190\n",
      "Iter: 231, running avg loss: 1.1387\n",
      "Iter: 232, running avg loss: 1.1540\n",
      "Iter: 233, running avg loss: 1.1095\n",
      "Iter: 234, running avg loss: 1.0843\n",
      "Iter: 235, running avg loss: 1.0617\n",
      "Iter: 236, running avg loss: 1.0740\n",
      "Iter: 237, running avg loss: 1.0737\n",
      "Iter: 238, running avg loss: 1.0865\n",
      "Iter: 239, running avg loss: 1.0685\n",
      "Iter: 240, running avg loss: 1.0994\n",
      "Iter: 241, running avg loss: 1.0158\n",
      "Iter: 242, running avg loss: 1.0622\n",
      "Iter: 243, running avg loss: 1.0383\n",
      "Iter: 244, running avg loss: 1.0198\n",
      "Iter: 245, running avg loss: 0.9627\n",
      "Iter: 246, running avg loss: 1.0732\n",
      "Iter: 247, running avg loss: 1.0567\n",
      "Iter: 248, running avg loss: 1.0229\n",
      "Iter: 249, running avg loss: 1.0269\n",
      "Iter: 250, running avg loss: 1.0454\n",
      "Iter: 251, running avg loss: 0.9992\n",
      "Iter: 252, running avg loss: 1.0203\n",
      "Iter: 253, running avg loss: 1.0024\n",
      "Iter: 254, running avg loss: 0.9958\n",
      "Iter: 255, running avg loss: 1.0406\n",
      "Iter: 256, running avg loss: 0.9702\n",
      "Iter: 257, running avg loss: 0.9829\n",
      "Iter: 258, running avg loss: 0.9408\n",
      "Iter: 259, running avg loss: 0.8960\n",
      "Iter: 260, running avg loss: 1.0554\n",
      "Iter: 261, running avg loss: 1.0074\n",
      "Iter: 262, running avg loss: 1.0592\n",
      "Iter: 263, running avg loss: 1.0105\n",
      "Iter: 264, running avg loss: 0.9338\n",
      "Iter: 265, running avg loss: 0.9210\n",
      "Iter: 266, running avg loss: 1.0380\n",
      "Iter: 267, running avg loss: 1.0068\n",
      "Iter: 268, running avg loss: 0.9909\n",
      "Iter: 269, running avg loss: 0.9504\n",
      "Iter: 270, running avg loss: 0.9713\n",
      "Iter: 271, running avg loss: 0.9734\n",
      "Iter: 272, running avg loss: 0.9101\n",
      "Iter: 273, running avg loss: 0.9325\n",
      "Iter: 274, running avg loss: 0.8832\n",
      "Iter: 275, running avg loss: 0.9066\n",
      "Iter: 276, running avg loss: 0.8798\n",
      "Iter: 277, running avg loss: 0.9555\n",
      "Iter: 278, running avg loss: 0.9050\n",
      "Iter: 279, running avg loss: 0.9514\n",
      "Iter: 280, running avg loss: 0.8650\n",
      "Iter: 281, running avg loss: 0.8531\n",
      "Iter: 282, running avg loss: 0.9874\n",
      "Iter: 283, running avg loss: 0.9275\n",
      "Iter: 284, running avg loss: 0.9132\n",
      "Iter: 285, running avg loss: 0.9115\n",
      "Iter: 286, running avg loss: 1.0030\n",
      "Iter: 287, running avg loss: 0.8920\n",
      "Iter: 288, running avg loss: 0.9066\n",
      "Iter: 289, running avg loss: 0.8840\n",
      "Iter: 290, running avg loss: 0.9618\n",
      "Iter: 291, running avg loss: 0.8900\n",
      "Iter: 292, running avg loss: 0.9308\n",
      "Iter: 293, running avg loss: 0.8677\n",
      "Iter: 294, running avg loss: 0.8689\n",
      "Iter: 295, running avg loss: 0.8414\n",
      "Iter: 296, running avg loss: 0.8638\n",
      "Iter: 297, running avg loss: 0.8994\n",
      "Iter: 298, running avg loss: 0.8571\n",
      "Iter: 299, running avg loss: 0.8396\n",
      "Iter: 300, running avg loss: 0.8846\n",
      "Iter: 301, running avg loss: 0.8770\n",
      "Iter: 302, running avg loss: 0.8402\n",
      "Iter: 303, running avg loss: 0.9125\n",
      "Iter: 304, running avg loss: 0.9388\n",
      "Iter: 305, running avg loss: 0.8742\n",
      "Iter: 306, running avg loss: 0.8717\n",
      "Iter: 307, running avg loss: 0.8540\n",
      "Iter: 308, running avg loss: 0.8330\n",
      "Iter: 309, running avg loss: 0.8606\n",
      "Iter: 310, running avg loss: 0.8847\n",
      "Iter: 311, running avg loss: 0.8361\n",
      "Iter: 312, running avg loss: 0.8626\n",
      "Iter: 313, running avg loss: 0.8213\n",
      "Iter: 314, running avg loss: 0.9036\n",
      "Iter: 315, running avg loss: 0.8272\n",
      "Iter: 316, running avg loss: 0.8266\n",
      "Iter: 317, running avg loss: 0.8024\n",
      "Iter: 318, running avg loss: 0.8604\n",
      "Iter: 319, running avg loss: 0.8458\n",
      "Iter: 320, running avg loss: 0.8644\n",
      "Iter: 321, running avg loss: 0.8934\n",
      "Iter: 322, running avg loss: 0.8434\n",
      "Iter: 323, running avg loss: 0.8210\n",
      "Iter: 324, running avg loss: 0.8514\n",
      "Iter: 325, running avg loss: 0.8179\n",
      "Iter: 326, running avg loss: 0.8537\n",
      "Iter: 327, running avg loss: 0.8610\n",
      "Iter: 328, running avg loss: 0.7832\n",
      "Iter: 329, running avg loss: 0.8433\n",
      "Iter: 330, running avg loss: 0.7892\n",
      "Iter: 331, running avg loss: 0.8469\n",
      "Iter: 332, running avg loss: 0.8676\n",
      "Iter: 333, running avg loss: 0.8835\n",
      "Iter: 334, running avg loss: 0.8319\n",
      "Iter: 335, running avg loss: 0.8589\n",
      "Iter: 336, running avg loss: 0.7900\n",
      "Iter: 337, running avg loss: 0.7955\n",
      "Iter: 338, running avg loss: 0.8330\n",
      "Iter: 339, running avg loss: 0.7776\n",
      "Iter: 340, running avg loss: 0.7836\n",
      "Iter: 341, running avg loss: 0.7540\n",
      "Iter: 342, running avg loss: 0.8472\n",
      "Iter: 343, running avg loss: 0.8061\n",
      "Iter: 344, running avg loss: 0.7733\n",
      "Iter: 345, running avg loss: 0.8159\n",
      "Iter: 346, running avg loss: 0.8409\n",
      "Iter: 347, running avg loss: 0.8315\n",
      "Iter: 348, running avg loss: 0.8307\n",
      "Iter: 349, running avg loss: 0.8295\n",
      "Iter: 350, running avg loss: 0.8282\n",
      "Iter: 351, running avg loss: 0.8020\n",
      "Iter: 352, running avg loss: 0.7678\n",
      "Iter: 353, running avg loss: 0.8781\n",
      "Iter: 354, running avg loss: 0.7748\n",
      "Iter: 355, running avg loss: 0.8234\n",
      "Iter: 356, running avg loss: 0.8366\n",
      "Iter: 357, running avg loss: 0.7353\n",
      "Iter: 358, running avg loss: 0.8434\n",
      "Iter: 359, running avg loss: 0.7441\n",
      "Iter: 360, running avg loss: 0.8411\n",
      "Iter: 361, running avg loss: 0.8214\n",
      "Iter: 362, running avg loss: 0.7543\n",
      "Iter: 363, running avg loss: 0.7701\n",
      "Iter: 364, running avg loss: 0.8443\n",
      "Iter: 365, running avg loss: 0.7519\n",
      "Iter: 366, running avg loss: 0.7858\n",
      "Iter: 367, running avg loss: 0.7465\n",
      "Iter: 368, running avg loss: 0.8464\n",
      "Iter: 369, running avg loss: 0.7840\n",
      "Iter: 370, running avg loss: 0.7303\n",
      "Iter: 371, running avg loss: 0.7903\n",
      "Iter: 372, running avg loss: 0.7437\n",
      "Iter: 373, running avg loss: 0.7626\n",
      "Iter: 374, running avg loss: 0.7933\n",
      "Iter: 375, running avg loss: 0.8177\n",
      "Iter: 376, running avg loss: 0.7642\n",
      "Iter: 377, running avg loss: 0.7231\n",
      "Iter: 378, running avg loss: 0.7822\n",
      "Iter: 379, running avg loss: 0.8058\n",
      "Iter: 380, running avg loss: 0.7571\n",
      "Iter: 381, running avg loss: 0.7357\n",
      "Iter: 382, running avg loss: 0.7838\n",
      "Iter: 383, running avg loss: 0.7658\n",
      "Iter: 384, running avg loss: 0.8317\n",
      "Iter: 385, running avg loss: 0.8376\n",
      "Iter: 386, running avg loss: 0.7399\n",
      "Iter: 387, running avg loss: 0.7863\n",
      "Iter: 388, running avg loss: 0.7687\n",
      "Iter: 389, running avg loss: 0.7359\n",
      "Iter: 390, running avg loss: 0.7584\n",
      "Iter: 391, running avg loss: 0.7642\n",
      "Iter: 392, running avg loss: 0.7643\n",
      "Iter: 393, running avg loss: 0.7504\n",
      "Iter: 394, running avg loss: 0.7354\n",
      "Iter: 395, running avg loss: 0.6918\n",
      "Iter: 396, running avg loss: 0.7510\n",
      "Iter: 397, running avg loss: 0.7559\n",
      "Iter: 398, running avg loss: 0.7457\n",
      "Iter: 399, running avg loss: 0.7450\n",
      "Iter: 400, running avg loss: 0.7510\n",
      "Iter: 401, running avg loss: 0.7644\n",
      "Iter: 402, running avg loss: 0.7334\n",
      "Iter: 403, running avg loss: 0.7380\n",
      "Iter: 404, running avg loss: 0.7425\n",
      "Iter: 405, running avg loss: 0.7143\n",
      "Iter: 406, running avg loss: 0.7622\n",
      "Iter: 407, running avg loss: 0.7232\n",
      "Iter: 408, running avg loss: 0.7005\n",
      "Iter: 409, running avg loss: 0.7689\n",
      "Iter: 410, running avg loss: 0.7034\n",
      "Iter: 411, running avg loss: 0.7288\n",
      "Iter: 412, running avg loss: 0.7316\n",
      "Iter: 413, running avg loss: 0.7463\n",
      "Iter: 414, running avg loss: 0.7394\n",
      "Iter: 415, running avg loss: 0.7253\n",
      "Iter: 416, running avg loss: 0.7380\n",
      "Iter: 417, running avg loss: 0.7553\n",
      "Iter: 418, running avg loss: 0.7187\n",
      "Iter: 419, running avg loss: 0.7239\n",
      "Iter: 420, running avg loss: 0.7331\n",
      "Iter: 421, running avg loss: 0.6613\n",
      "Iter: 422, running avg loss: 0.7440\n",
      "Iter: 423, running avg loss: 0.6723\n",
      "Iter: 424, running avg loss: 0.6848\n",
      "Iter: 425, running avg loss: 0.7120\n",
      "Iter: 426, running avg loss: 0.7318\n",
      "Iter: 427, running avg loss: 0.6959\n",
      "Iter: 428, running avg loss: 0.7438\n",
      "Iter: 429, running avg loss: 0.7401\n",
      "Iter: 430, running avg loss: 0.7062\n",
      "Iter: 431, running avg loss: 0.6923\n",
      "Iter: 432, running avg loss: 0.6688\n",
      "Iter: 433, running avg loss: 0.7209\n",
      "Iter: 434, running avg loss: 0.6487\n",
      "Iter: 435, running avg loss: 0.7780\n",
      "Iter: 436, running avg loss: 0.6084\n",
      "Iter: 437, running avg loss: 0.7362\n",
      "Iter: 438, running avg loss: 0.7192\n",
      "Iter: 439, running avg loss: 0.6879\n",
      "Iter: 440, running avg loss: 0.6875\n",
      "Iter: 441, running avg loss: 0.6688\n",
      "Iter: 442, running avg loss: 0.7357\n",
      "Iter: 443, running avg loss: 0.7014\n",
      "Iter: 444, running avg loss: 0.6932\n",
      "Iter: 445, running avg loss: 0.7152\n",
      "Iter: 446, running avg loss: 0.6638\n",
      "Iter: 447, running avg loss: 0.7140\n",
      "Iter: 448, running avg loss: 0.6938\n",
      "Iter: 449, running avg loss: 0.6877\n",
      "Iter: 450, running avg loss: 0.6646\n",
      "Iter: 451, running avg loss: 0.6871\n",
      "Iter: 452, running avg loss: 0.6937\n",
      "Iter: 453, running avg loss: 0.7033\n",
      "Iter: 454, running avg loss: 0.6946\n",
      "Iter: 455, running avg loss: 0.6833\n",
      "Iter: 456, running avg loss: 0.6590\n",
      "Iter: 457, running avg loss: 0.6844\n",
      "Iter: 458, running avg loss: 0.6545\n",
      "Iter: 459, running avg loss: 0.6699\n",
      "Iter: 460, running avg loss: 0.6682\n",
      "Iter: 461, running avg loss: 0.6985\n",
      "Iter: 462, running avg loss: 0.6726\n",
      "Iter: 463, running avg loss: 0.6821\n",
      "Iter: 464, running avg loss: 0.6808\n",
      "Iter: 465, running avg loss: 0.6999\n",
      "Iter: 466, running avg loss: 0.6675\n",
      "Iter: 467, running avg loss: 0.6518\n",
      "Iter: 468, running avg loss: 0.6904\n",
      "Iter: 469, running avg loss: 0.7000\n",
      "Iter: 470, running avg loss: 0.7213\n",
      "Iter: 471, running avg loss: 0.7370\n",
      "Iter: 472, running avg loss: 0.6772\n",
      "Iter: 473, running avg loss: 0.6838\n",
      "Iter: 474, running avg loss: 0.7181\n",
      "Iter: 475, running avg loss: 0.7033\n",
      "Iter: 476, running avg loss: 0.7007\n",
      "Iter: 477, running avg loss: 0.7040\n",
      "Iter: 478, running avg loss: 0.6798\n",
      "Iter: 479, running avg loss: 0.7413\n",
      "Iter: 480, running avg loss: 0.6926\n",
      "Iter: 481, running avg loss: 0.7289\n",
      "Iter: 482, running avg loss: 0.7057\n",
      "Iter: 483, running avg loss: 0.6971\n",
      "Iter: 484, running avg loss: 0.6898\n",
      "Iter: 485, running avg loss: 0.6447\n",
      "Iter: 486, running avg loss: 0.6955\n",
      "Iter: 487, running avg loss: 0.6536\n",
      "Iter: 488, running avg loss: 0.6912\n",
      "Iter: 489, running avg loss: 0.6836\n",
      "Iter: 490, running avg loss: 0.6847\n",
      "Iter: 491, running avg loss: 0.6681\n",
      "Iter: 492, running avg loss: 0.6503\n",
      "Iter: 493, running avg loss: 0.6769\n",
      "Iter: 494, running avg loss: 0.6776\n",
      "Iter: 495, running avg loss: 0.6873\n",
      "Iter: 496, running avg loss: 0.6346\n",
      "Iter: 497, running avg loss: 0.6559\n",
      "Iter: 498, running avg loss: 0.6523\n",
      "Iter: 499, running avg loss: 0.6382\n",
      "Iter: 500, running avg loss: 0.6601\n",
      "Iter: 501, running avg loss: 0.6660\n",
      "Iter: 502, running avg loss: 0.6449\n",
      "Iter: 503, running avg loss: 0.6809\n",
      "Iter: 504, running avg loss: 0.6485\n",
      "Iter: 505, running avg loss: 0.6204\n",
      "Iter: 506, running avg loss: 0.6128\n",
      "Iter: 507, running avg loss: 0.7208\n",
      "Iter: 508, running avg loss: 0.6874\n",
      "Iter: 509, running avg loss: 0.6762\n",
      "Iter: 510, running avg loss: 0.6717\n",
      "Iter: 511, running avg loss: 0.6457\n",
      "Iter: 512, running avg loss: 0.6591\n",
      "Iter: 513, running avg loss: 0.6880\n",
      "Iter: 514, running avg loss: 0.6934\n",
      "Iter: 515, running avg loss: 0.6755\n",
      "Iter: 516, running avg loss: 0.6860\n",
      "Iter: 517, running avg loss: 0.6613\n",
      "Iter: 518, running avg loss: 0.6499\n",
      "Iter: 519, running avg loss: 0.6511\n",
      "Iter: 520, running avg loss: 0.7345\n",
      "Iter: 521, running avg loss: 0.6741\n",
      "Iter: 522, running avg loss: 0.7006\n",
      "Iter: 523, running avg loss: 0.6196\n",
      "Iter: 524, running avg loss: 0.6566\n",
      "Iter: 525, running avg loss: 0.6687\n",
      "Iter: 526, running avg loss: 0.6261\n",
      "Iter: 527, running avg loss: 0.6700\n",
      "Iter: 528, running avg loss: 0.6702\n",
      "Iter: 529, running avg loss: 0.6633\n",
      "Iter: 530, running avg loss: 0.7119\n",
      "Iter: 531, running avg loss: 0.6591\n",
      "Iter: 532, running avg loss: 0.6577\n",
      "Iter: 533, running avg loss: 0.6565\n",
      "Iter: 534, running avg loss: 0.6264\n",
      "Iter: 535, running avg loss: 0.7073\n",
      "Iter: 536, running avg loss: 0.6775\n",
      "Iter: 537, running avg loss: 0.7135\n",
      "Iter: 538, running avg loss: 0.6004\n",
      "Iter: 539, running avg loss: 0.6752\n",
      "Iter: 540, running avg loss: 0.7345\n",
      "Iter: 541, running avg loss: 0.6643\n",
      "Iter: 542, running avg loss: 0.6746\n",
      "Iter: 543, running avg loss: 0.7206\n",
      "Iter: 544, running avg loss: 0.6742\n",
      "Iter: 545, running avg loss: 0.6790\n",
      "Iter: 546, running avg loss: 0.6498\n",
      "Iter: 547, running avg loss: 0.6900\n",
      "Iter: 548, running avg loss: 0.6510\n",
      "Iter: 549, running avg loss: 0.6974\n",
      "Iter: 550, running avg loss: 0.7058\n",
      "Iter: 551, running avg loss: 0.7182\n",
      "Iter: 552, running avg loss: 0.6080\n",
      "Iter: 553, running avg loss: 0.7256\n",
      "Iter: 554, running avg loss: 0.6188\n",
      "Iter: 555, running avg loss: 0.6689\n",
      "Iter: 556, running avg loss: 0.7379\n",
      "Iter: 557, running avg loss: 0.6576\n",
      "Iter: 558, running avg loss: 0.6959\n",
      "Iter: 559, running avg loss: 0.7611\n",
      "Iter: 560, running avg loss: 0.6407\n",
      "Iter: 561, running avg loss: 0.7917\n",
      "Iter: 562, running avg loss: 0.6195\n",
      "Iter: 563, running avg loss: 0.7135\n",
      "Iter: 564, running avg loss: 0.7155\n",
      "Iter: 565, running avg loss: 0.6657\n",
      "Iter: 566, running avg loss: 0.6769\n",
      "Iter: 567, running avg loss: 0.6621\n",
      "Iter: 568, running avg loss: 0.6654\n",
      "Iter: 569, running avg loss: 0.7132\n",
      "Iter: 570, running avg loss: 0.7549\n",
      "Iter: 571, running avg loss: 0.6499\n",
      "Iter: 572, running avg loss: 0.7359\n",
      "Iter: 573, running avg loss: 0.6627\n",
      "Iter: 574, running avg loss: 0.6837\n",
      "Iter: 575, running avg loss: 0.7038\n",
      "Iter: 576, running avg loss: 0.6786\n",
      "Iter: 577, running avg loss: 0.6369\n",
      "Iter: 578, running avg loss: 0.6689\n",
      "Iter: 579, running avg loss: 0.6795\n",
      "Iter: 580, running avg loss: 0.6462\n",
      "Iter: 581, running avg loss: 0.7140\n",
      "Iter: 582, running avg loss: 0.6008\n",
      "Iter: 583, running avg loss: 0.6253\n",
      "Iter: 584, running avg loss: 0.6796\n",
      "Iter: 585, running avg loss: 0.6972\n",
      "Iter: 586, running avg loss: 0.6595\n",
      "Iter: 587, running avg loss: 0.5947\n",
      "Iter: 588, running avg loss: 0.6612\n",
      "Iter: 589, running avg loss: 0.6723\n",
      "Iter: 590, running avg loss: 0.6111\n",
      "Iter: 591, running avg loss: 0.6243\n",
      "Iter: 592, running avg loss: 0.5866\n",
      "Iter: 593, running avg loss: 0.6341\n",
      "Iter: 594, running avg loss: 0.6722\n",
      "Iter: 595, running avg loss: 0.6509\n",
      "Iter: 596, running avg loss: 0.6153\n",
      "Iter: 597, running avg loss: 0.6532\n",
      "Iter: 598, running avg loss: 0.6047\n",
      "Iter: 599, running avg loss: 0.6512\n",
      "Iter: 600, running avg loss: 0.7044\n",
      "Iter: 601, running avg loss: 0.6662\n",
      "Iter: 602, running avg loss: 0.6388\n",
      "Iter: 603, running avg loss: 0.6160\n",
      "Iter: 604, running avg loss: 0.6647\n",
      "Iter: 605, running avg loss: 0.6318\n",
      "Iter: 606, running avg loss: 0.6220\n",
      "Iter: 607, running avg loss: 0.6817\n",
      "Iter: 608, running avg loss: 0.5962\n",
      "Iter: 609, running avg loss: 0.6600\n",
      "Iter: 610, running avg loss: 0.6473\n",
      "Iter: 611, running avg loss: 0.6315\n",
      "Iter: 612, running avg loss: 0.6687\n",
      "Iter: 613, running avg loss: 0.6574\n",
      "Iter: 614, running avg loss: 0.6256\n",
      "Iter: 615, running avg loss: 0.6082\n",
      "Iter: 616, running avg loss: 0.6411\n",
      "Iter: 617, running avg loss: 0.5894\n",
      "Iter: 618, running avg loss: 0.6053\n",
      "Iter: 619, running avg loss: 0.6389\n",
      "Iter: 620, running avg loss: 0.6149\n",
      "Iter: 621, running avg loss: 0.5956\n",
      "Iter: 622, running avg loss: 0.6165\n",
      "Iter: 623, running avg loss: 0.5693\n",
      "Iter: 624, running avg loss: 0.6260\n",
      "Iter: 625, running avg loss: 0.5930\n",
      "Iter: 626, running avg loss: 0.6335\n",
      "Iter: 627, running avg loss: 0.6156\n",
      "Iter: 628, running avg loss: 0.5962\n",
      "Iter: 629, running avg loss: 0.6768\n",
      "Iter: 630, running avg loss: 0.6449\n",
      "Iter: 631, running avg loss: 0.6573\n",
      "Iter: 632, running avg loss: 0.6960\n",
      "Iter: 633, running avg loss: 0.6786\n",
      "Iter: 634, running avg loss: 0.7001\n",
      "Iter: 635, running avg loss: 0.6335\n",
      "Iter: 636, running avg loss: 0.6532\n",
      "Iter: 637, running avg loss: 0.5901\n",
      "Iter: 638, running avg loss: 0.6269\n",
      "Iter: 639, running avg loss: 0.6154\n",
      "Iter: 640, running avg loss: 0.6161\n",
      "Iter: 641, running avg loss: 0.6016\n",
      "Iter: 642, running avg loss: 0.6868\n",
      "Iter: 643, running avg loss: 0.6019\n",
      "Iter: 644, running avg loss: 0.6748\n",
      "Iter: 645, running avg loss: 0.6301\n",
      "Iter: 646, running avg loss: 0.6353\n",
      "Iter: 647, running avg loss: 0.7252\n",
      "Iter: 648, running avg loss: 0.6923\n",
      "Iter: 649, running avg loss: 0.6050\n",
      "Iter: 650, running avg loss: 0.7390\n",
      "Iter: 651, running avg loss: 0.5996\n",
      "Iter: 652, running avg loss: 0.6329\n",
      "Iter: 653, running avg loss: 0.6924\n",
      "Iter: 654, running avg loss: 0.6091\n",
      "Iter: 655, running avg loss: 0.6316\n",
      "Iter: 656, running avg loss: 0.6219\n",
      "Iter: 657, running avg loss: 0.5786\n",
      "Iter: 658, running avg loss: 0.7297\n",
      "Iter: 659, running avg loss: 0.6562\n",
      "Iter: 660, running avg loss: 0.6389\n",
      "Iter: 661, running avg loss: 0.6265\n",
      "Iter: 662, running avg loss: 0.6488\n",
      "Iter: 663, running avg loss: 0.6362\n",
      "Iter: 664, running avg loss: 0.6160\n",
      "Iter: 665, running avg loss: 0.5776\n",
      "Iter: 666, running avg loss: 0.6048\n",
      "Iter: 667, running avg loss: 0.5985\n",
      "Iter: 668, running avg loss: 0.5866\n",
      "Iter: 669, running avg loss: 0.6262\n",
      "Iter: 670, running avg loss: 0.6496\n",
      "Iter: 671, running avg loss: 0.5740\n",
      "Iter: 672, running avg loss: 0.5926\n",
      "Iter: 673, running avg loss: 0.5663\n",
      "Iter: 674, running avg loss: 0.6062\n",
      "Iter: 675, running avg loss: 0.5643\n",
      "Iter: 676, running avg loss: 0.5991\n",
      "Iter: 677, running avg loss: 0.6889\n",
      "Iter: 678, running avg loss: 0.6109\n",
      "Iter: 679, running avg loss: 0.6757\n",
      "Iter: 680, running avg loss: 0.5868\n",
      "Iter: 681, running avg loss: 0.6530\n",
      "Iter: 682, running avg loss: 0.6070\n",
      "Iter: 683, running avg loss: 0.6273\n",
      "Iter: 684, running avg loss: 0.6308\n",
      "Iter: 685, running avg loss: 0.5552\n",
      "Iter: 686, running avg loss: 0.6580\n",
      "Iter: 687, running avg loss: 0.6013\n",
      "Iter: 688, running avg loss: 0.6575\n",
      "Iter: 689, running avg loss: 0.5974\n",
      "Iter: 690, running avg loss: 0.5854\n",
      "Iter: 691, running avg loss: 0.6407\n",
      "Iter: 692, running avg loss: 0.5869\n",
      "Iter: 693, running avg loss: 0.6840\n",
      "Iter: 694, running avg loss: 0.5934\n",
      "Iter: 695, running avg loss: 0.6102\n",
      "Iter: 696, running avg loss: 0.6079\n",
      "Iter: 697, running avg loss: 0.6130\n",
      "Iter: 698, running avg loss: 0.6595\n",
      "Iter: 699, running avg loss: 0.5744\n",
      "Iter: 700, running avg loss: 0.6387\n",
      "Iter: 701, running avg loss: 0.6564\n",
      "Iter: 702, running avg loss: 0.6128\n",
      "Iter: 703, running avg loss: 0.6399\n",
      "Iter: 704, running avg loss: 0.6038\n",
      "Iter: 705, running avg loss: 0.6563\n",
      "Iter: 706, running avg loss: 0.6362\n",
      "Iter: 707, running avg loss: 0.6208\n",
      "Iter: 708, running avg loss: 0.5950\n",
      "Iter: 709, running avg loss: 0.6237\n",
      "Iter: 710, running avg loss: 0.6519\n",
      "Iter: 711, running avg loss: 0.6111\n",
      "Iter: 712, running avg loss: 0.6361\n",
      "Iter: 713, running avg loss: 0.6182\n",
      "Iter: 714, running avg loss: 0.5878\n",
      "Iter: 715, running avg loss: 0.6537\n",
      "Iter: 716, running avg loss: 0.5637\n",
      "Iter: 717, running avg loss: 0.5989\n",
      "Iter: 718, running avg loss: 0.6512\n",
      "Iter: 719, running avg loss: 0.6087\n",
      "Iter: 720, running avg loss: 0.6684\n",
      "Iter: 721, running avg loss: 0.6285\n",
      "Iter: 722, running avg loss: 0.6228\n",
      "Iter: 723, running avg loss: 0.5915\n",
      "Iter: 724, running avg loss: 0.6552\n",
      "Iter: 725, running avg loss: 0.5979\n",
      "Iter: 726, running avg loss: 0.6563\n",
      "Iter: 727, running avg loss: 0.6308\n",
      "Iter: 728, running avg loss: 0.6005\n",
      "Iter: 729, running avg loss: 0.6833\n",
      "Iter: 730, running avg loss: 0.6180\n",
      "Iter: 731, running avg loss: 0.5830\n",
      "Iter: 732, running avg loss: 0.6255\n",
      "Iter: 733, running avg loss: 0.6533\n",
      "Iter: 734, running avg loss: 0.6606\n",
      "Iter: 735, running avg loss: 0.6640\n",
      "Iter: 736, running avg loss: 0.6546\n",
      "Iter: 737, running avg loss: 0.6452\n",
      "Iter: 738, running avg loss: 0.6230\n",
      "Iter: 739, running avg loss: 0.5772\n",
      "Iter: 740, running avg loss: 0.6584\n",
      "Iter: 741, running avg loss: 0.6553\n",
      "Iter: 742, running avg loss: 0.5456\n",
      "Iter: 743, running avg loss: 0.6629\n",
      "Iter: 744, running avg loss: 0.5720\n",
      "Iter: 745, running avg loss: 0.6105\n",
      "Iter: 746, running avg loss: 0.6368\n",
      "Iter: 747, running avg loss: 0.5952\n",
      "Iter: 748, running avg loss: 0.6359\n",
      "Iter: 749, running avg loss: 0.6154\n",
      "Iter: 750, running avg loss: 0.5880\n",
      "Iter: 751, running avg loss: 0.6009\n",
      "Iter: 752, running avg loss: 0.6466\n",
      "Iter: 753, running avg loss: 0.6667\n",
      "Iter: 754, running avg loss: 0.6517\n",
      "Iter: 755, running avg loss: 0.6237\n",
      "Iter: 756, running avg loss: 0.6083\n",
      "Iter: 757, running avg loss: 0.5914\n",
      "Iter: 758, running avg loss: 0.6228\n",
      "Iter: 759, running avg loss: 0.6480\n",
      "Iter: 760, running avg loss: 0.6378\n",
      "Iter: 761, running avg loss: 0.5644\n",
      "Iter: 762, running avg loss: 0.6118\n",
      "Iter: 763, running avg loss: 0.6215\n",
      "Iter: 764, running avg loss: 0.6431\n",
      "Iter: 765, running avg loss: 0.6752\n",
      "Iter: 766, running avg loss: 0.6021\n",
      "Iter: 767, running avg loss: 0.6611\n",
      "Iter: 768, running avg loss: 0.5872\n",
      "Iter: 769, running avg loss: 0.6220\n",
      "Iter: 770, running avg loss: 0.6129\n",
      "Iter: 771, running avg loss: 0.5613\n",
      "Iter: 772, running avg loss: 0.6546\n",
      "Iter: 773, running avg loss: 0.6103\n",
      "Iter: 774, running avg loss: 0.5996\n",
      "Iter: 775, running avg loss: 0.6541\n",
      "Iter: 776, running avg loss: 0.5981\n",
      "Iter: 777, running avg loss: 0.6142\n",
      "Iter: 778, running avg loss: 0.5392\n",
      "Iter: 779, running avg loss: 0.5738\n",
      "Iter: 780, running avg loss: 0.5957\n",
      "Iter: 781, running avg loss: 0.5750\n",
      "Iter: 782, running avg loss: 0.6218\n",
      "Iter: 783, running avg loss: 0.5467\n",
      "Iter: 784, running avg loss: 0.6117\n",
      "Iter: 785, running avg loss: 0.6483\n",
      "Iter: 786, running avg loss: 0.6023\n",
      "Iter: 787, running avg loss: 0.6333\n",
      "Iter: 788, running avg loss: 0.5949\n",
      "Iter: 789, running avg loss: 0.5940\n",
      "Iter: 790, running avg loss: 0.6043\n",
      "Iter: 791, running avg loss: 0.6410\n",
      "Iter: 792, running avg loss: 0.6094\n",
      "Iter: 793, running avg loss: 0.5569\n",
      "Iter: 794, running avg loss: 0.5873\n",
      "Iter: 795, running avg loss: 0.5514\n",
      "Iter: 796, running avg loss: 0.6156\n",
      "Iter: 797, running avg loss: 0.5793\n",
      "Iter: 798, running avg loss: 0.5745\n",
      "Iter: 799, running avg loss: 0.5895\n",
      "Iter: 800, running avg loss: 0.6030\n",
      "Iter: 801, running avg loss: 0.5689\n",
      "Iter: 802, running avg loss: 0.6020\n",
      "Iter: 803, running avg loss: 0.6729\n",
      "Iter: 804, running avg loss: 0.5825\n",
      "Iter: 805, running avg loss: 0.6135\n",
      "Iter: 806, running avg loss: 0.5743\n",
      "Iter: 807, running avg loss: 0.5909\n",
      "Iter: 808, running avg loss: 0.5701\n",
      "Iter: 809, running avg loss: 0.6510\n",
      "Iter: 810, running avg loss: 0.5371\n",
      "Iter: 811, running avg loss: 0.6444\n",
      "Iter: 812, running avg loss: 0.6082\n",
      "Iter: 813, running avg loss: 0.6007\n",
      "Iter: 814, running avg loss: 0.5702\n",
      "Iter: 815, running avg loss: 0.5441\n",
      "Iter: 816, running avg loss: 0.5924\n",
      "Iter: 817, running avg loss: 0.5729\n",
      "Iter: 818, running avg loss: 0.6061\n",
      "Iter: 819, running avg loss: 0.6327\n",
      "Iter: 820, running avg loss: 0.5849\n",
      "Iter: 821, running avg loss: 0.5378\n",
      "Iter: 822, running avg loss: 0.6532\n",
      "Iter: 823, running avg loss: 0.5876\n",
      "Iter: 824, running avg loss: 0.6385\n",
      "Iter: 825, running avg loss: 0.5887\n",
      "Iter: 826, running avg loss: 0.5950\n",
      "Iter: 827, running avg loss: 0.6122\n",
      "Iter: 828, running avg loss: 0.5809\n",
      "Iter: 829, running avg loss: 0.6291\n",
      "Iter: 830, running avg loss: 0.6251\n",
      "Iter: 831, running avg loss: 0.6388\n",
      "Iter: 832, running avg loss: 0.5252\n",
      "Iter: 833, running avg loss: 0.6213\n",
      "Iter: 834, running avg loss: 0.5923\n",
      "Iter: 835, running avg loss: 0.5725\n",
      "Iter: 836, running avg loss: 0.6599\n",
      "Iter: 837, running avg loss: 0.5574\n",
      "Iter: 838, running avg loss: 0.6782\n",
      "Iter: 839, running avg loss: 0.5958\n",
      "Iter: 840, running avg loss: 0.5192\n",
      "Iter: 841, running avg loss: 0.6644\n",
      "Iter: 842, running avg loss: 0.6318\n",
      "Iter: 843, running avg loss: 0.6335\n",
      "Iter: 844, running avg loss: 0.6907\n",
      "Iter: 845, running avg loss: 0.5909\n",
      "Iter: 846, running avg loss: 0.7055\n",
      "Iter: 847, running avg loss: 0.6669\n",
      "Iter: 848, running avg loss: 0.6040\n",
      "Iter: 849, running avg loss: 0.7349\n",
      "Iter: 850, running avg loss: 0.6624\n",
      "Iter: 851, running avg loss: 0.6137\n",
      "Iter: 852, running avg loss: 0.7252\n",
      "Iter: 853, running avg loss: 0.6426\n",
      "Iter: 854, running avg loss: 0.6627\n",
      "Iter: 855, running avg loss: 0.6891\n",
      "Iter: 856, running avg loss: 0.6057\n",
      "Iter: 857, running avg loss: 0.7303\n",
      "Iter: 858, running avg loss: 0.6273\n",
      "Iter: 859, running avg loss: 0.5922\n",
      "Iter: 860, running avg loss: 0.6395\n",
      "Iter: 861, running avg loss: 0.6596\n",
      "Iter: 862, running avg loss: 0.6230\n",
      "Iter: 863, running avg loss: 0.6058\n",
      "Iter: 864, running avg loss: 0.5884\n",
      "Iter: 865, running avg loss: 0.6170\n",
      "Iter: 866, running avg loss: 0.6748\n",
      "Iter: 867, running avg loss: 0.6576\n",
      "Iter: 868, running avg loss: 0.5743\n",
      "Iter: 869, running avg loss: 0.6820\n",
      "Iter: 870, running avg loss: 0.5606\n",
      "Iter: 871, running avg loss: 0.5919\n",
      "Iter: 872, running avg loss: 0.5713\n",
      "Iter: 873, running avg loss: 0.6493\n",
      "Iter: 874, running avg loss: 0.5938\n",
      "Iter: 875, running avg loss: 0.5960\n",
      "Iter: 876, running avg loss: 0.6047\n",
      "Iter: 877, running avg loss: 0.5435\n",
      "Iter: 878, running avg loss: 0.6287\n",
      "Iter: 879, running avg loss: 0.5837\n",
      "Iter: 880, running avg loss: 0.6158\n",
      "Iter: 881, running avg loss: 0.5558\n",
      "Iter: 882, running avg loss: 0.6024\n",
      "Iter: 883, running avg loss: 0.6051\n",
      "Iter: 884, running avg loss: 0.5568\n",
      "Iter: 885, running avg loss: 0.5429\n",
      "Iter: 886, running avg loss: 0.5495\n",
      "Iter: 887, running avg loss: 0.6266\n",
      "Iter: 888, running avg loss: 0.5933\n",
      "Iter: 889, running avg loss: 0.6368\n",
      "Iter: 890, running avg loss: 0.5945\n",
      "Iter: 891, running avg loss: 0.6753\n",
      "Iter: 892, running avg loss: 0.6171\n",
      "Iter: 893, running avg loss: 0.6179\n",
      "Iter: 894, running avg loss: 0.6184\n",
      "Iter: 895, running avg loss: 0.5578\n",
      "Iter: 896, running avg loss: 0.6454\n",
      "Iter: 897, running avg loss: 0.5800\n",
      "Iter: 898, running avg loss: 0.5732\n",
      "Iter: 899, running avg loss: 0.6287\n",
      "Iter: 900, running avg loss: 0.6105\n",
      "Iter: 901, running avg loss: 0.5399\n",
      "Iter: 902, running avg loss: 0.6060\n",
      "Iter: 903, running avg loss: 0.5580\n",
      "Iter: 904, running avg loss: 0.6075\n",
      "Iter: 905, running avg loss: 0.6206\n",
      "Iter: 906, running avg loss: 0.6295\n",
      "Iter: 907, running avg loss: 0.5390\n",
      "Iter: 908, running avg loss: 0.5574\n",
      "Iter: 909, running avg loss: 0.6306\n",
      "Iter: 910, running avg loss: 0.5738\n",
      "Iter: 911, running avg loss: 0.5623\n",
      "Iter: 912, running avg loss: 0.5500\n",
      "Iter: 913, running avg loss: 0.6004\n",
      "Iter: 914, running avg loss: 0.5823\n",
      "Iter: 915, running avg loss: 0.5839\n",
      "Iter: 916, running avg loss: 0.6280\n",
      "Iter: 917, running avg loss: 0.6007\n",
      "Iter: 918, running avg loss: 0.5524\n",
      "Iter: 919, running avg loss: 0.6305\n",
      "Iter: 920, running avg loss: 0.6080\n",
      "Iter: 921, running avg loss: 0.6045\n",
      "Iter: 922, running avg loss: 0.5718\n",
      "Iter: 923, running avg loss: 0.5887\n",
      "Iter: 924, running avg loss: 0.6428\n",
      "Iter: 925, running avg loss: 0.6696\n",
      "Iter: 926, running avg loss: 0.6272\n",
      "Iter: 927, running avg loss: 0.6755\n",
      "Iter: 928, running avg loss: 0.6197\n",
      "Iter: 929, running avg loss: 0.5408\n",
      "Iter: 930, running avg loss: 0.6016\n",
      "Iter: 931, running avg loss: 0.5374\n",
      "Iter: 932, running avg loss: 0.5690\n",
      "Iter: 933, running avg loss: 0.5801\n",
      "Iter: 934, running avg loss: 0.5754\n",
      "Iter: 935, running avg loss: 0.5759\n",
      "Iter: 936, running avg loss: 0.5829\n",
      "Iter: 937, running avg loss: 0.6126\n",
      "Iter: 938, running avg loss: 0.5707\n",
      "Iter: 939, running avg loss: 0.6027\n",
      "Iter: 940, running avg loss: 0.6349\n",
      "Iter: 941, running avg loss: 0.6535\n",
      "Iter: 942, running avg loss: 0.5564\n",
      "Iter: 943, running avg loss: 0.6863\n",
      "Iter: 944, running avg loss: 0.6046\n",
      "Iter: 945, running avg loss: 0.5881\n",
      "Iter: 946, running avg loss: 0.6314\n",
      "Iter: 947, running avg loss: 0.5271\n",
      "Iter: 948, running avg loss: 0.5964\n",
      "Iter: 949, running avg loss: 0.5613\n",
      "Iter: 950, running avg loss: 0.6198\n",
      "Iter: 951, running avg loss: 0.6193\n",
      "Iter: 952, running avg loss: 0.5861\n",
      "Iter: 953, running avg loss: 0.5692\n",
      "Iter: 954, running avg loss: 0.5492\n",
      "Iter: 955, running avg loss: 0.6505\n",
      "Iter: 956, running avg loss: 0.6142\n",
      "Iter: 957, running avg loss: 0.5581\n",
      "Iter: 958, running avg loss: 0.6088\n",
      "Iter: 959, running avg loss: 0.5565\n",
      "Iter: 960, running avg loss: 0.5346\n",
      "Iter: 961, running avg loss: 0.6065\n",
      "Iter: 962, running avg loss: 0.6161\n",
      "Iter: 963, running avg loss: 0.5717\n",
      "Iter: 964, running avg loss: 0.5516\n",
      "Iter: 965, running avg loss: 0.5741\n",
      "Iter: 966, running avg loss: 0.6349\n",
      "Iter: 967, running avg loss: 0.6484\n",
      "Iter: 968, running avg loss: 0.5990\n",
      "Iter: 969, running avg loss: 0.6208\n",
      "Iter: 970, running avg loss: 0.6621\n",
      "Iter: 971, running avg loss: 0.5967\n",
      "Iter: 972, running avg loss: 0.6769\n",
      "Iter: 973, running avg loss: 0.6251\n",
      "Iter: 974, running avg loss: 0.6621\n",
      "Iter: 975, running avg loss: 0.5958\n",
      "Iter: 976, running avg loss: 0.5852\n",
      "Iter: 977, running avg loss: 0.6434\n",
      "Iter: 978, running avg loss: 0.5845\n",
      "Iter: 979, running avg loss: 0.5659\n",
      "Iter: 980, running avg loss: 0.5991\n",
      "Iter: 981, running avg loss: 0.5635\n",
      "Iter: 982, running avg loss: 0.6155\n",
      "Iter: 983, running avg loss: 0.6107\n",
      "Iter: 984, running avg loss: 0.5570\n",
      "Iter: 985, running avg loss: 0.6412\n",
      "Iter: 986, running avg loss: 0.6295\n",
      "Iter: 987, running avg loss: 0.5865\n",
      "Iter: 988, running avg loss: 0.6006\n",
      "Iter: 989, running avg loss: 0.5825\n",
      "Iter: 990, running avg loss: 0.5796\n",
      "Iter: 991, running avg loss: 0.5914\n",
      "Iter: 992, running avg loss: 0.5624\n",
      "Iter: 993, running avg loss: 0.5953\n",
      "Iter: 994, running avg loss: 0.6183\n",
      "Iter: 995, running avg loss: 0.5844\n",
      "Iter: 996, running avg loss: 0.5619\n",
      "Iter: 997, running avg loss: 0.5425\n",
      "Iter: 998, running avg loss: 0.7183\n",
      "Iter: 999, running avg loss: 0.6004\n",
      "Iter: 1000, running avg loss: 0.5737\n"
     ]
    }
   ],
   "source": [
    "t0 = 0\n",
    "t1 = 10\n",
    "device = torch.device('cuda:0')\n",
    "args=ARGS()\n",
    "# model\n",
    "func = CNF(in_out_dim=2, hidden_dim=args.hidden_dim, width=args.width).to(device)\n",
    "optimizer = optim.Adam(func.parameters(), lr=args.lr)\n",
    "p_z0 = torch.distributions.MultivariateNormal(\n",
    "    loc=torch.tensor([0.0, 0.0]).to(device),\n",
    "    covariance_matrix=torch.tensor([[0.1, 0.0], [0.0, 0.1]]).to(device)\n",
    ")\n",
    "loss_hist = []\n",
    "\n",
    "# if args.train_dir is not None:\n",
    "#     if not os.path.exists(args.train_dir):\n",
    "#         os.makedirs(args.train_dir)\n",
    "#     ckpt_path = os.path.join(args.train_dir, 'ckpt.pth')\n",
    "#     if os.path.exists(ckpt_path):\n",
    "#         checkpoint = torch.load(ckpt_path)\n",
    "#         func.load_state_dict(checkpoint['func_state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         print('Loaded ckpt from {}'.format(ckpt_path))\n",
    "\n",
    "\n",
    "for itr in range(1, args.niters + 1):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, logp_diff_t1 = get_batch(args.num_samples)\n",
    "\n",
    "    z_t, logp_diff_t = odeint(\n",
    "        func,\n",
    "        (x, logp_diff_t1),\n",
    "        torch.tensor([t1, t0]).type(torch.float32).to(device),\n",
    "        atol=1e-5,\n",
    "        rtol=1e-5,\n",
    "        method='dopri5',\n",
    "    )\n",
    "\n",
    "    z_t0, logp_diff_t0 = z_t[-1], logp_diff_t[-1]\n",
    "\n",
    "    logp_x = p_z0.log_prob(z_t0).to(device) - logp_diff_t0.view(-1)\n",
    "    loss = -logp_x.mean(0)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_hist.append(loss.item())\n",
    "\n",
    "    print('Iter: {}, running avg loss: {:.4f}'.format(itr, loss_hist[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "if args.viz:\n",
    "    viz_samples = 30000\n",
    "    viz_timesteps = 9\n",
    "    target_sample, _ = get_batch(viz_samples)\n",
    "\n",
    "    if not os.path.exists(args.save):\n",
    "        os.makedirs(args.save)\n",
    "    with torch.no_grad():\n",
    "        # Generate evolution of samples\n",
    "        z_t0 = p_z0.sample([viz_samples]).to(device)\n",
    "        logp_diff_t0 = torch.zeros(viz_samples, 1).type(torch.float32).to(device)\n",
    "\n",
    "        z_t_samples, _ = odeint(\n",
    "            func,\n",
    "            (z_t0, logp_diff_t0),\n",
    "            torch.tensor(np.linspace(t0, t1, viz_timesteps)).to(device),\n",
    "            atol=1e-5,\n",
    "            rtol=1e-5,\n",
    "            method='rk4',\n",
    "        )\n",
    "\n",
    "        # Generate evolution of density\n",
    "        x = np.linspace(-2, 2, 100)\n",
    "        y = np.linspace(-2, 2, 100)\n",
    "        points = np.vstack(np.meshgrid(x, y)).reshape([2, -1]).T\n",
    "\n",
    "        z_t1 = torch.tensor(points).type(torch.float32).to(device)\n",
    "        logp_diff_t1 = torch.zeros(z_t1.shape[0], 1).type(torch.float32).to(device)\n",
    "\n",
    "        z_t_density, logp_diff_t = odeint(\n",
    "            func,\n",
    "            (z_t1, logp_diff_t1),\n",
    "            torch.tensor(np.linspace(t1, t0, viz_timesteps)).to(device),\n",
    "            atol=1e-5,\n",
    "            rtol=1e-5,\n",
    "            method='rk4',\n",
    "        )\n",
    "\n",
    "        # Create plots for each timestep\n",
    "        plt.figure(figsize=(25, 4))\n",
    "        # plt.show()\n",
    "        ii = 1\n",
    "        for (t, z_sample, z_density, logp_diff) in zip(\n",
    "                np.linspace(t0, t1, viz_timesteps),\n",
    "                z_t_samples, z_t_density, logp_diff_t\n",
    "        ):\n",
    "            # fig = plt.figure(figsize=(12, 4), dpi=200)\n",
    "            # plt.tight_layout()\n",
    "            # plt.axis('off')\n",
    "            # plt.margins(0, 0)\n",
    "            # fig.suptitle(f'{t:.2f}s')\n",
    "\n",
    "            # ax1 = fig.add_subplot(1, 3, 1)\n",
    "            # ax1.set_title('Target')\n",
    "            # ax1.get_xaxis().set_ticks([])\n",
    "            # ax1.get_yaxis().set_ticks([])\n",
    "            # ax2 = fig.add_subplot(1, 3, 2)\n",
    "            # ax2.set_title('Samples')\n",
    "            # ax2.get_xaxis().set_ticks([])\n",
    "            # ax2.get_yaxis().set_ticks([])\n",
    "            # ax3 = fig.add_subplot(1, 3, 3)\n",
    "            # ax3.set_title('Log Probability')\n",
    "            # ax3.get_xaxis().set_ticks([])\n",
    "            # ax3.get_yaxis().set_ticks([])\n",
    "\n",
    "            # ax1.hist2d(*target_sample.detach().cpu().numpy().T, bins=300, density=True,\n",
    "            #             range=[[-2, 2], [-2, 2]],cmap='Greys')\n",
    "\n",
    "            # ax2.hist2d(*z_sample.detach().cpu().numpy().T, bins=300, density=True,\n",
    "            #             range=[[-2, 2], [-2, 2]],cmap='Greys')\n",
    "\n",
    "            logp = p_z0.log_prob(z_density) - logp_diff.view(-1)\n",
    "            # ax3.tricontourf(*z_t1.detach().cpu().numpy().T,\n",
    "            #                 np.exp(logp.detach().cpu().numpy()), 200,cmap='Greys')\n",
    "\n",
    "            # plt.savefig(os.path.join(args.save, f\"cnf-viz-{int(t*1000):05d}.jpg\"),\n",
    "            #             pad_inches=0.2, bbox_inches='tight')\n",
    "            # plt.close()\n",
    "\n",
    "            # print(ii)\n",
    "            plt.subplot(1,10,ii)\n",
    "            plt.axis('off')\n",
    "            plt.title(f't={t:.2f}')\n",
    "            plt.hist2d(*z_sample.detach().cpu().numpy().T, bins=300, density=True,range=[[-2, 3], [-1.5, 1.5]],cmap='Greys')\n",
    "            ii += 1\n",
    "        # img, *imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(args.save, f\"cnf-viz-*.jpg\")))]\n",
    "        # img.save(fp=os.path.join(args.save, \"cnf-viz.gif\"), format='GIF', append_images=imgs,\n",
    "        #             save_all=True, duration=250, loop=0)\n",
    "        # plt.show()\n",
    "        plt.subplot(1,viz_timesteps,viz_timesteps)\n",
    "        plt.axis('off')\n",
    "        plt.title('Target')\n",
    "        plt.hist2d(*target_sample.detach().cpu().numpy().T, bins=300, density=True,range=[[-2, 3], [-1.5, 1.5]],cmap='Greys')\n",
    "    # print('Saved visualization animation at {}'.format(os.path.join(args.save, \"cnf-viz.gif\")))\n",
    "    plt.savefig(os.path.join('cnf', \"cnf-viz.pdf\"), bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
